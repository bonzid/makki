{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Extraction des données du lexique libanais de Makki\n",
        "\n",
        "Ce document permet de transformer vos données (format docx) en un format exportable sur la plateforme du Lexique libanais de Makki en tant qu'administrateur (format rdf)."
      ],
      "metadata": {
        "id": "eV9U-v3N9UQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation des extensions"
      ],
      "metadata": {
        "id": "1502bamq897X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx==1.1.0\n",
        "!pip install isodate==0.6.1\n",
        "!pip install lxml==5.1.0\n",
        "!pip install pillow\n",
        "!pip install pyparsing==3.1.1\n",
        "!pip install rdflib==7.0.0\n",
        "!pip install six\n",
        "!pip install typing-extensions==4.9.0"
      ],
      "metadata": {
        "id": "_Yz3wHNFB9us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pour docx vers json\n",
        "import json\n",
        "from docx import Document\n",
        "import re\n",
        "\n",
        "#Pour json vers rdf\n",
        "import os\n",
        "\n",
        "#Pour rdf_id\n",
        "import uuid\n",
        "import hashlib\n",
        "\n",
        "#Pour main\n",
        "import argparse\n",
        "import time\n",
        "import sys"
      ],
      "metadata": {
        "id": "ErDMn7zu9ryv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformation d'un fichier docx en un fichier json"
      ],
      "metadata": {
        "id": "r3n72VLi9tg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_to_uuid(*data: tuple[str])->uuid.UUID:\n",
        "    s = hashlib.md5(bytes(\".\".join(list(data)), 'utf-8'))\n",
        "    return uuid.UUID(bytes=s.digest())"
      ],
      "metadata": {
        "id": "SdSkgEDqNVaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtractData:\n",
        "\n",
        "    prefixes_fr = {'[Imper.]', '[Jeux]', '[Alim.]', '[Bot.]', '[Zool.]', '[Zool. ; ois.]', '[Zool./Mar. ; poiss.]', '[Zool. ; insec.]', '[Méd./Mal.]', '[Mus.]', '[Mus. ; instr.]', '[Loc. ; vulg.]', '[Pop. ; vulg.]', '[Vulg.]', '[Compar. ; vulg.]'}\n",
        "    prefixes_ar = {'[لِلأمر]', '[ألعاب]', '[غِذاء]', '[نَبات]', '[حَيَوان]', '[حَيَوان؛ طُيُور]', '[حَيَوانٌ بَـحرِيّ؛ أسـماك]', '[حَيَوان؛ حَشَرات]', '[طِبّ]', '[مُوسِيقى]', '[مُوسِيقى؛ آلَة]', '[عِبارَة؛ سُوقِيّ]', '[شَعبِيّ؛ سُوقِيّ]', '[سُوقِيّ]', '[لِلـمُقارَنَة؛ سُوقِيّ]'}\n",
        "    themes_ok_fr={'Jeux', 'Zoologie','Médecine','Botanique', 'Technique','Alimentation'}\n",
        "    themes_ok_ar={'ألعاب','حَيَوانٌ', 'طِبّ','نَبات','تِقَنِيّات','غِذاء'}\n",
        "\n",
        "    cols_lang = [\"ar\", \"ar\", \"fr\", \"ar\"]\n",
        "\n",
        "    regex_coverage = re.compile(r\"(\\((?P<def>[\\d])\\) |)((?P<first_loc>[\\w ]+), |)(((?P<loc1>[\\w]+)( (\\((?P<prec1>.+)\\))|) (&|et) (?P<loc2>[\\w]+)( \\((?P<prec2>.+)\\)|))|(?P<loc_glob>[\\w ]+) (\\((?P<loc>[\\w ]+)\\))|(?P<loca>[\\w ]+[\\w]+)( (\\((?P<prec3>.+)\\))|))( : (?P<precision>.+[^\\n ]{1})|)\", re.M | re.U)\n",
        "    regex_definition = re.compile(r\"^(?P<head>(?P<nb>[0-9]\\.) |(\\((?P<symbol>.{1})\\) )| ?® ?|)((?P<tag>\\[.+\\]) |)(- (?P<example>.+?)|(?P<def>.+?)) *$\", re.M | re.U)\n",
        "    #regex_definition = re.compile(r\"^(?P<head>(?P<nb>[0-9]\\.) |(\\((?P<symbol>.*)\\) )| ?® ?|)((?P<tag>\\[.+\\]) |)(- (?P<example>.+?)|(?P<def>.+?)) *$\", re.M | re.U)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, file_path, annotations_file_path):\n",
        "        self.file_path = file_path\n",
        "        self.annotations_file_path = annotations_file_path\n",
        "        self.data = {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate_term_uri(self, description_id):\n",
        "        return str(data_to_uuid(description_id))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def make_term(self, terme, description_fr, description_ar):\n",
        "        data = {}\n",
        "\n",
        "        # On rassemble les informations des descriptions en français et en arabe\n",
        "        for key in description_fr.keys() | description_ar.keys():\n",
        "\n",
        "            if key in description_fr and key in description_ar:\n",
        "                data[key] = description_fr[key] + description_ar[key]\n",
        "            elif key in description_fr:\n",
        "                data[key] = description_fr[key]\n",
        "            else:\n",
        "                data[key] = description_ar[key]\n",
        "\n",
        "        data[\"title\"] = terme\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def parse_term(self, s: str):\n",
        "        # On découpe la case du mot en séparant avec les tirets\n",
        "        s = [el.strip() for el in s.split('-')]\n",
        "\n",
        "        # On isole le mot lui-même de ses exemples\n",
        "        #NOTE: on perd actuellement les informations de pluriel, de féminin, etc.\n",
        "        return re.match(r\"[^\\d\\(\\)\\n\\r\\f]+\", s[0]).group(0).strip(), [(ex, \"ar\") for ex in s[1:]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def parse_coverage(self, s: str, lang: str):\n",
        "        output = []\n",
        "        s = s.lower()\n",
        "        # On utilise la regex `regex_coverage` pour découper la chaîne en différentes informations\n",
        "        for m in self.regex_coverage.finditer(s.strip()):\n",
        "\n",
        "            prec = m['precision']\n",
        "\n",
        "            if m['first_loc']:\n",
        "                output.append((m['first_loc'], lang, prec))\n",
        "\n",
        "            elif m['loc1']:\n",
        "                output.append((m['loc1'].lower(), lang, m['prec1'] if m['prec1'] else prec))\n",
        "                output.append((m['loc2'].lower(), lang, m['prec2'] if m['prec2'] else prec))\n",
        "\n",
        "            elif m['loc_glob']:\n",
        "                output.append((m['loc_glob'].lower(), lang, prec))\n",
        "                output.append((m['loc'].lower(), lang, prec))\n",
        "\n",
        "            elif m['loca']:\n",
        "                output.append((m['loca'].lower(), lang, m['prec3'] if m['prec3'] else prec))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def parse_definition(self, s: str, tags: set, lang: str):\n",
        "        output = {\"abstract\" : [\"\"]}\n",
        "        output['quic'] = False\n",
        "\n",
        "        valid_themes = self.themes_ok_fr if lang == \"fr\" else self.themes_ok_ar\n",
        "\n",
        "        # On utilise la regex `regex_definition` pour découper la définition en éléments reconnus\n",
        "        for d in self.regex_definition.finditer(s):\n",
        "\n",
        "            d = d.groupdict()\n",
        "            #print(f\"Détection : {d}\")\n",
        "\n",
        "\n",
        "\n",
        "            # On traite chaque cas possible\n",
        "            if d['symbol'] == '*':\n",
        "                output['abstract'][0] += '\\n(*)' + d['def'].strip()\n",
        "\n",
        "            elif d['symbol'] == '+':\n",
        "                if 'related' not in output:\n",
        "                    output['related'] = []\n",
        "                output['related'].append((d['def'].strip(), lang))\n",
        "\n",
        "\n",
        "            elif d['symbol'] == 'ε':\n",
        "\n",
        "                if 'etymo' not in output:\n",
        "                    output['etymo'] = []\n",
        "\n",
        "                output['etymo'].append((d['def'].strip(), lang))\n",
        "\n",
        "            elif d['symbol'] == 'π':\n",
        "                if 'pron' not in output:\n",
        "                    output['pron'] = []\n",
        "                output['pron'].append((d['def'].strip(), lang))\n",
        "\n",
        "            elif d['head'].strip() == \"®\":\n",
        "                if lang == \"fr\":\n",
        "                    if 'coverage' not in output:\n",
        "                        output['coverage'] = []\n",
        "                    output['coverage'] += self.parse_coverage(d['def'], lang)\n",
        "                else:\n",
        "                    output['abstract'][0] += '\\n' + d['def'].strip()\n",
        "\n",
        "\n",
        "            elif not d['symbol']:\n",
        "                if d['def']:\n",
        "\n",
        "                    if d['tag'] and d['tag'] != '':\n",
        "                      #print(f\"Tag extrait : {d['tag']}\")\n",
        "                      #tg = d['tag'].split('/')[0].split(';')[0].strip().rstrip('[]')\n",
        "                      tg = d['tag'].split('/')[0].split(';')[0].strip().strip('[]')\n",
        "                      #print(f\"Tag modifié : {tg}\")  # Debug\n",
        "\n",
        "\n",
        "                      #print(f\"Tag extrait : {tg}, Thèmes valides : {valid_themes}\")\n",
        "\n",
        "                      # Ne traiter que les tags correspondant aux thèmes valides\n",
        "                      if tg in valid_themes:\n",
        "                          if tg not in tags:\n",
        "                              if 'subject' not in output:\n",
        "                                  output['subject'] = []\n",
        "                              output['subject'].append((tg[0:].strip(), lang))\n",
        "                          else:\n",
        "                              output['abstract'][0] += d['tag'] + ' '\n",
        "\n",
        "                    output['abstract'][0] += d['def']\n",
        "\n",
        "                elif d['example']:\n",
        "                    if 'example' not in output:\n",
        "                            output['example'] = []\n",
        "\n",
        "                    output['example'].append((d['example'].strip(), lang))\n",
        "\n",
        "        if d['def'] is not None and \"¤\" in d['def']:\n",
        "          output['quic'] = True\n",
        "        output['abstract'][0] = (output['abstract'][0], lang)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def parse_row(self, row, tags):\n",
        "        #print([cell.text for cell in row.cells])\n",
        "\n",
        "        if len(row.cells) > 2 and row.cells[0].text.strip() != \"\" and row.cells[2].text.strip() != \"\":\n",
        "\n",
        "            # On itère sur les lignes qui ont un mot de présent dans la première colonne\n",
        "            if row.cells[0].text.strip() != \"\" and row.cells[2].text.strip() != \"\":\n",
        "\n",
        "                # On récupère le terme et les examples\n",
        "                terme, examples = self.parse_term(row.cells[0].text)\n",
        "\n",
        "                # On récupère chaque description et les tags valides associés (= pas dans annotations.txt)\n",
        "                description_fr = self.parse_definition(row.cells[2].text, tags, \"fr\")\n",
        "                description_ar = self.parse_definition(row.cells[1].text, tags, \"ar\")\n",
        "\n",
        "                # On ajoute le terme au dictionnaire ainsi que les définitions associées\n",
        "                term_uri = self.generate_term_uri(description_fr['abstract'][0][0])\n",
        "\n",
        "                if not terme in self.data:\n",
        "                    self.data[terme] = {\"description\" : {}}\n",
        "\n",
        "                self.data[terme][\"description\"][term_uri] = self.make_term(terme, description_fr, description_ar)\n",
        "\n",
        "\n",
        "                # On ajoute les exemples\n",
        "                if len(examples) > 0:\n",
        "                    if not 'example' in self.data[terme][\"description\"][term_uri]:\n",
        "                        self.data[terme][\"description\"][term_uri]['example'] = []\n",
        "\n",
        "                    self.data[terme][\"description\"][term_uri][\"example\"] += examples\n",
        "\n",
        "                # On ajoute les synonymes en leur donnant la même définition que le mot actuel\n",
        "                if (alternatif := re.match(r\"(\\([0-9]\\)|) ?← (.*)\", row.cells[3].text)) != None:\n",
        "\n",
        "                    for alt in alternatif.group(2).split(\"،\"):\n",
        "                        alt = alt.strip()\n",
        "                        if alt != \"\":\n",
        "                            if not alt in self.data:\n",
        "                                self.data[alt] = {\"description\" : {}}\n",
        "\n",
        "                            self.data[alt][\"description\"][term_uri] = self.make_term(alt, description_fr, description_ar)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def extract_definition(self, verbose=True):\n",
        "        # On récupère le fichier de données et l'annexe des tags à éliminer\n",
        "        document = Document(self.file_path)\n",
        "        tags, symbols = self.read_annotations_file()\n",
        "        # TODO: prendre en compte les symboles\n",
        "        # On itère sur les tableaux du document\n",
        "        for i, table in enumerate(document.tables):\n",
        "            # Si l'on a suffisamment de colonnes, on itère sur les lignes\n",
        "            if len(table.columns) >= 4:\n",
        "                for i, row in enumerate(table.rows):\n",
        "                    if verbose:\n",
        "                        print(f\"\\t\\t| {i+1:05} / {len(table.rows):05} |\")\n",
        "                    self.parse_row(row, tags)\n",
        "\n",
        "\n",
        "\n",
        "    def read_annotations_file(self):\n",
        "        tags = []\n",
        "        symbols = []\n",
        "\n",
        "        try:\n",
        "            with open(self.annotations_file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
        "                annotations_to_delete = file.readlines()\n",
        "\n",
        "                for annotation in annotations_to_delete:\n",
        "                    if not (annotation[0] in {'[', '('}):\n",
        "                        symbols.append(annotation.strip())\n",
        "                    else:\n",
        "                        tags.append(annotation.strip())\n",
        "        except:\n",
        "            print(\"Fichier d'exclusion INVALIDE :\", self.annotations_file_path)\n",
        "            pass\n",
        "\n",
        "        return (set(tags), set(symbols))\n",
        "\n",
        "    def write_json_to_file(self, file_path):\n",
        "        with open(file_path, mode=\"w\", encoding=\"utf-8\") as file:\n",
        "            json.dump(self.data, file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "OrCK4YfYyAE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformation d'un fichier json en un fichier rdf"
      ],
      "metadata": {
        "id": "zvog-PcQ91QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_rdf(input_json, source_file_name):\n",
        "    rdf_prefixes = \"\"\"    @prefix rdf:   <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
        "    @prefix rdfs:  <http://www.w3.org/2000/01/rdf-schema#> .\n",
        "    @prefix xsd:   <http://www.w3.org/2001/XMLSchema#> .\n",
        "    @prefix dc:    <http://purl.org/dc/terms/> .\n",
        "    @prefix lex:   <lexique/> .\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    rdf_template = \"\"\"<lexique/{title_uuid}#{identifier}>\n",
        "        dc:creator \"{creator}\" ;\n",
        "        dc:publisher \"{publisher}\" ;\n",
        "        dc:identifier \"{identifier}\" ;\n",
        "        dc:source \"{source}\" ;\n",
        "        dc:title \"{title}\"@ar ;\n",
        "        {coverage}\n",
        "        {example}\n",
        "        {etymo}\n",
        "        {subject}\n",
        "        {quic}\n",
        "        {abstract} .\n",
        "        \"\"\"\n",
        "\n",
        "    with open(input_json, mode='r', encoding='utf-8') as json_file:\n",
        "        data = json.load(json_file)\n",
        "\n",
        "    source_file_name = os.path.basename(source_file_name)\n",
        "\n",
        "    rdf_output = rdf_prefixes + '\\n'.join([\n",
        "        rdf_template.format(\n",
        "\n",
        "            title_uuid=data_to_uuid(title),\n",
        "\n",
        "            title = title,\n",
        "\n",
        "            identifier=lexeme_id,\n",
        "\n",
        "            creator=\"Hassan Makki\",\n",
        "\n",
        "            publisher=\"Editions Geuthner\",\n",
        "\n",
        "            coverage=';\\n'.join([f'dc:coverage \"{escape(subj[0])}\"@fr' for subj in lexeme_info.get(\"coverage\", [])]) + ' ;' if lexeme_info.get(\"coverage\") else \"\",\n",
        "\n",
        "            subject=\";\\n\".join([f'dc:subject \"{escape(subject[0])}\"@{subject[1]}' for subject in lexeme_info.get(\"subject\", [])]) + ' ;' if lexeme_info.get(\"subject\") else \"\",\n",
        "\n",
        "            example=\";\\n\".join([f'lex:example \\\"\\\"\\\"{escape(example[0])}\\\"\\\"\\\"@{example[1]}' for example in lexeme_info.get(\"example\", [])]) + ' ;' if lexeme_info.get(\"example\") else \"\",\n",
        "\n",
        "            source= source_file_name,\n",
        "\n",
        "            etymo=\";\\n\".join([f'lex:etymo \"{escape(etymo[0])}\"@{etymo[1]}' for etymo in lexeme_info.get(\"etymo\", [])])+ ' ;' if lexeme_info.get(\"etymo\") else \"\",\n",
        "\n",
        "            quic=f'dc:quic \"{str(bool(lexeme_info.get(\"quic\"))).lower()}\"'+ ' ;',\n",
        "\n",
        "\n",
        "            abstract=\";\\n\".join([f'dc:abstract \\\"\\\"\\\"{escape(abstract[0])}\\\"\\\"\\\"@{abstract[1]}' for abstract in lexeme_info.get(\"abstract\", [])]) if lexeme_info.get(\"abstract\") else \"\"\n",
        "        )\n",
        "\n",
        "        for title, lexeme_data in data.items()\n",
        "        for lexeme_id, lexeme_info in lexeme_data[\"description\"].items()\n",
        "    ])\n",
        "\n",
        "    return rdf_output\n",
        "\n",
        "\n",
        "def escape(s: str) -> str:\n",
        "\n",
        "    out = \"\"\n",
        "\n",
        "    for i in range(len(s)):\n",
        "\n",
        "        match s[i]:\n",
        "            case '\\n':\n",
        "                out += '\\\\n'\n",
        "            case '\\r':\n",
        "                out += '\\\\r'\n",
        "            case '\\t' :\n",
        "                out += '\\\\t'\n",
        "            case '\\f':\n",
        "                out += '\\\\f'\n",
        "            case '\"' | \"'\":\n",
        "                out += '\\\\' + s[i]\n",
        "            case _:\n",
        "                out += s[i]\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "CXv_1V2x99bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utilisation du script\n",
        "Ici, vous exécutez le script ci-dessous avec :\n",
        "- En entrée : le chemin vers votre fichier docx.\n",
        "- En sortie : le chemin vers votre futur fichier rdf."
      ],
      "metadata": {
        "id": "D3xbV3Kj-BdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VERBOSE=False\n",
        "\n",
        "def echo(s: str):\n",
        "    if VERBOSE:\n",
        "        print(s)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    #Arguments simulés\n",
        "\n",
        "\n",
        "\n",
        "    #--------------------CHANGEMENTS A FAIRE--------------------\n",
        "    args_list=[\n",
        "        \"/content/kha.docx\",  #--------------- ICI, CHANGER LE NOM DU FICHIER\n",
        "        \"--output\", \"/content/\",  #Dossier de sortie -- ne pas changer\n",
        "        \"--verbose\",  #Affichage détaillé -- ne pas changer\n",
        "    ]\n",
        "    #--------------------FIN CHANGEMENTS A FAIRE--------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    parser=argparse.ArgumentParser(\n",
        "        prog='Extracteur Makki',\n",
        "        description=\"Récupère les informations d'un fichier docx et en tire un fichier rdf (turtle).\")\n",
        "\n",
        "    parser.add_argument('filename', help=\"Chemin des fichiers docx d'entrée\", nargs='+')\n",
        "    parser.add_argument('--exclude', \"-x\", required=False, help=\"Chemin vers un fichier des tags à exclure (optionnel)\", nargs=1, default=\"\")\n",
        "    parser.add_argument('--output', '-o', required=True, help=\"Dossier(s) vers lesquels exporter les données. Soit 1 seul, soit 1 par fichier d'entrée\", nargs='+')\n",
        "    parser.add_argument('--verbose', '-v', required=False, help=\"Affiche la progression du système.\", action=\"store_true\")\n",
        "    parser.add_argument('--rdf', \"-r\", required=False, help=\"Stipule que les fichiers d'entrée sont déjà des fichiers json valides\", action=\"store_true\")\n",
        "\n",
        "    # Utilisation des arguments définis par args_list\n",
        "    args=parser.parse_args(args_list)\n",
        "\n",
        "    if len(args.output) != 1:\n",
        "        if len(args.output) != len(args.filename):\n",
        "            raise argparse.ArgumentError(\"Il faut soit un seul dossier de sortie, soit un dossier par fichier d'entrée.\")\n",
        "        else:\n",
        "            output_folder = args.output # several\n",
        "    else:\n",
        "        output_folder = [args.output[0]]*len(args.filename) # unique\n",
        "\n",
        "    VERBOSE = args.verbose\n",
        "\n",
        "    for f, d in zip(args.filename, output_folder):\n",
        "\n",
        "        output_name = os.path.basename(f).split('.')[0]\n",
        "        json_path = os.path.join(d, output_name + \".json\")\n",
        "\n",
        "        if not args.rdf:\n",
        "\n",
        "            ed = ExtractData(f, args.exclude[0] if len(args.exclude) > 0 else \"\")\n",
        "\n",
        "            echo(f\"| Extraction du fichier `{f}`...\")\n",
        "            t0 = time.time()\n",
        "            ed.extract_definition(VERBOSE)\n",
        "            t1 = time.time()\n",
        "\n",
        "            echo(f\"\\t> {t1-t0} s ({(t1-t0)/60} min)\")\n",
        "\n",
        "            ed.write_json_to_file(json_path)\n",
        "\n",
        "        echo(f\"\\t> Export en RDF...\")\n",
        "        rdf = convert_to_rdf(json_path, output_name)\n",
        "\n",
        "        with open(os.path.join(d, output_name + '.ttl'), mode=\"w\", encoding=\"utf-8\") as ttl:\n",
        "            ttl.write(rdf)\n",
        "\n",
        "        echo(\"\\t> TERMINE\")\n",
        "\n",
        "    echo(\"TRAITEMENT TERMINE.\")"
      ],
      "metadata": {
        "id": "CICZ_lidMIqy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}